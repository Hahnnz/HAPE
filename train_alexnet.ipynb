{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scripts import tools\n",
    "from scripts import dataset\n",
    "from scripts.config import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "Joint_Classes = np.array([\"Right ankle\", \"Right knee\", \"Right hip\", \"Left hip\", \"Left knee\", \"Left ankle\", \"Right wrist\",\n",
    "                  \"Right elbow\", \"Right shoulder\", \"Left shoulder\", \"Left elbow\", \"Left wrist\", \"Neck\", \"Head top\"])\n",
    "\n",
    "MET_Classes = np.array([\"walking about\", \"writing\", \"reading.seated\", \"typing\", \"lifting.packing-lifting\", \"lifting.packing-packing\", \n",
    "                        \"filling.seated\", \"filling.stand\", \"cooking\", \"house cleaning\", \"machine work.light\", \"machine work.sawing\", \n",
    "                        \"reclining\", \"seated.quiet\", \"sleeping\", \"standing.relaxed\"])\n",
    "\n",
    "tools.etc.set_GPU(\"2,3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=96\n",
    "learning_rate = 0.01\n",
    "num_joints = len(Joint_Classes)\n",
    "num_met = len(MET_Classes)\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "PROJECT_ROOT = %pwd\n",
    "\n",
    "train_csv = PROJECT_ROOT + \"/dataset/train.csv\"\n",
    "test_csv = PROJECT_ROOT + \"/dataset/test.csv\"\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Processing Images & Coordinates]: 100%|██████████| 96/96 [00:01<00:00, 74.58it/s]\n"
     ]
    }
   ],
   "source": [
    "#train_data = dataset.iterator(csv_file=\"./dataset/train_MET_L.csv\", batch_size=batch_size, Shuffle=True)\n",
    "test_data = dataset.met(csv_file=\"./dataset/test_MET_L.csv\", Shuffle=True, one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-712cb672af6b>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models import alexnet\n",
    "\n",
    "model = alexnet.alexnet(batch_size, (227,227,3), (11,))\n",
    "score = model.fc8\n",
    "\n",
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]\n",
    "\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score,labels=model.y_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc6/weights:0 is illegal; using fc6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0 is illegal; using fc6/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=1,rho=0.95,epsilon=1e-09).minimize(loss)\n",
    "    \n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "    \n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(model.y_gt, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(filewriter_path)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_per_epoch = int(np.floor(train_data.img_set.shape.as_list()[0]/batch_size))\n",
    "num_val=test_data.img_set.shape.as_list()[0]\n",
    "val_batches_per_epoch = int(np.floor(num_val/ batch_size if num_val > batch_size else num_val))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "num_epochs = \n",
    "next_batch = train_data.iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epochs : 299 ] Train - Accuracy : 0.9942129585478041 Train - Loss : 0.022381499111159227 Test - Accuracy : 0.3905164925381541 Test - Loss : 7.472269892692566: 100%|██████████| 300/300 [1:24:30<00:00, 20.53s/it]  \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tqdm(total = num_epochs) as pbar:\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "            # Loop over number of epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # Initialize iterator with the training dataset\n",
    "                sess.run(train_data.init_op)\n",
    "\n",
    "                train_acc = 0.\n",
    "                train_loss = 0.\n",
    "                train_count = 0\n",
    "                for step in range(train_batches_per_epoch):\n",
    "                    # get next batch of data\n",
    "                    img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "                    # And run the training op\n",
    "                    acc,cost,op=sess.run([accuracy,loss,optimizer], feed_dict={model.x: img_batch,\n",
    "                                                  model.y_gt: label_batch,\n",
    "                                                  keep_prob: 0.7})\n",
    "                    # Generate summary with the current batch of data and write to file\n",
    "                    if step % display_step == 0:\n",
    "                        s = sess.run(merged_summary, feed_dict={model.x: img_batch,\n",
    "                                                                model.y_gt: label_batch,\n",
    "                                                                keep_prob: 0.7})\n",
    "\n",
    "                        writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                    train_acc += acc\n",
    "                    train_loss += cost\n",
    "                    train_count += 1\n",
    "                \n",
    "                #\"\"\"\n",
    "                # Validate the model on the entire validation set\n",
    "\n",
    "                sess.run(test_data.init_op)\n",
    "                test_acc = 0.\n",
    "                test_loss = 0.\n",
    "                test_count = 0\n",
    "                for _ in range(val_batches_per_epoch):\n",
    "                    acc,cost = sess.run([accuracy,loss], feed_dict={model.x: testtest.img_set,\n",
    "                                                                  model.y_gt: np.eye(np.max(testtest.labels)+1)[testtest.labels].reshape(96,11),\n",
    "                                                                  keep_prob: 0.7})\n",
    "                    test_acc += acc\n",
    "                    test_loss += cost\n",
    "                    test_count += 1\n",
    "                    \n",
    "                pbar.set_description(\"[Epochs : \"+str(epoch)+\" ]\"+\n",
    "                                     \" Train - Accuracy : \"+str(train_acc/train_count if train_count !=0 else 0)+\n",
    "                                     \" Train - Loss : \"+str(train_loss/train_count if train_count !=0 else 0)+\n",
    "                                     \" Test - Accuracy : \"+str(test_acc/test_count if test_count !=0 else 0)+\n",
    "                                     \" Test - Loss : \"+str(test_loss/test_count if test_count !=0 else 0))\n",
    "                pbar.update(1)\n",
    "                #\"\"\"\n",
    "                # save checkpoint of the model\n",
    "                checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "                save_path = saver.save(sess, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
