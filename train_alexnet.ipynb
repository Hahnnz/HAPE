{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scripts import tools\n",
    "from scripts import dataset\n",
    "from scripts.config import *\n",
    "from models import *\n",
    "from tensorflow.contrib.data import Iterator\n",
    "from tensorflow.contrib.data import Dataset\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow.python.framework.ops import convert_to_tensor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "Joint_Classes = np.array([\"Right ankle\", \"Right knee\", \"Right hip\", \"Left hip\", \"Left knee\", \"Left ankle\", \"Right wrist\",\n",
    "                  \"Right elbow\", \"Right shoulder\", \"Left shoulder\", \"Left elbow\", \"Left wrist\", \"Neck\", \"Head top\"])\n",
    "\n",
    "MET_Classes = np.array([\"walking about\", \"writing\", \"reading.seated\", \"typing\", \"lifting.packing-lifting\", \"lifting.packing-packing\", \n",
    "                        \"filling.seated\", \"filling.stand\", \"cooking\", \"house cleaning\", \"machine work.light\", \"machine work.sawing\", \n",
    "                        \"reclining\", \"seated.quiet\", \"sleeping\", \"standing.relaxed\"])\n",
    "\n",
    "tools.etc.set_GPU(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "learning_rate = 0.01\n",
    "num_joints = len(Joint_Classes)\n",
    "num_met = len(MET_Classes)\n",
    "\n",
    "display_step = 100\n",
    "\n",
    "PROJECT_ROOT = %pwd\n",
    "\n",
    "train_csv = PROJECT_ROOT + \"/dataset/train.csv\"\n",
    "test_csv = PROJECT_ROOT + \"/dataset/test.csv\"\n",
    "\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function_train(img, label):\n",
    "    return img, tf.one_hot(label, num_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Processing Images & Coordinates]: 100%|██████████| 864/864 [00:17<00:00, 48.60it/s] \n",
      "[Processing Images & Coordinates]: 100%|██████████| 96/96 [00:02<00:00, 44.74it/s] \n"
     ]
    }
   ],
   "source": [
    "train_data = dataset.met(csv_file=\"./dataset/train.csv\", Shuffle=True)\n",
    "test_data = dataset.met(csv_file=\"./dataset/test.csv\", Shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-3e5ba86036a1>:4: Dataset.from_tensor_slices (from tensorflow.contrib.data.python.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.from_tensor_slices()`.\n",
      "WARNING:tensorflow:From <ipython-input-5-3e5ba86036a1>:6: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with output_buffer_size is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n",
      "WARNING:tensorflow:From <ipython-input-5-3e5ba86036a1>:6: calling Dataset.map (from tensorflow.contrib.data.python.ops.dataset_ops) with num_threads is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Replace `num_threads=T` with `num_parallel_calls=T`. Replace `output_buffer_size=N` with `ds.prefetch(N)` on the returned dataset.\n"
     ]
    }
   ],
   "source": [
    "tr_img_set = convert_to_tensor(train_data.img_set, dtype=dtypes.float64)\n",
    "tr_labels = convert_to_tensor(train_data.labels[:,0], dtype= dtypes.int32)\n",
    "\n",
    "tr_data = Dataset.from_tensor_slices((tr_img_set, tr_labels))\n",
    "\n",
    "tr_data = tr_data.map(_parse_function_train, num_threads=8, output_buffer_size = 100* batch_size)\n",
    "tr_data = tr_data.batch(batch_size)\n",
    "\n",
    "tr_iterator = Iterator.from_structure(tr_data.output_types, tr_data.output_shapes)\n",
    "training_init_op = tr_iterator.make_initializer(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_set = convert_to_tensor(test_data.img_set, dtype=dtypes.float64)\n",
    "val_labels = convert_to_tensor(test_data.labels, dtype= dtypes.int32)\n",
    "\n",
    "val_data = Dataset.from_tensor_slices((val_img_set, val_labels))\n",
    "\n",
    "val_data = val_data.map(_parse_function_train, num_threads=8,\n",
    "                output_buffer_size=100*batch_size)\n",
    "val_data = val_data.batch(batch_size)\n",
    "\n",
    "val_iterator = Iterator.from_structure(val_data.output_types, val_data.output_shapes)\n",
    "validation_init_op = val_iterator.make_initializer(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alexnet.alexnet(batch_size, (227,227,3), (num_met,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.fc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_layers = ['fc8', 'fc7', 'fc6']\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] in train_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-3febc4476a62>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"cross_ent\"):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=score,labels=model.y_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=1,rho=0.95,epsilon=1e-09).minimize(loss)\n",
    "    #train_op = optimizer.apply_gradients(grads_and_vars=gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc6/weights:0/gradient is illegal; using fc6/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0/gradient is illegal; using fc6/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0/gradient is illegal; using fc7/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0/gradient is illegal; using fc7/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n"
     ]
    }
   ],
   "source": [
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name fc6/weights:0 is illegal; using fc6/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc6/biases:0 is illegal; using fc6/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/weights:0 is illegal; using fc7/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc7/biases:0 is illegal; using fc7/biases_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cross_entropy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('cross_entropy', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(score, 1), tf.argmax(model.y_gt, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(filewriter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_per_epoch = int(np.floor(len(train_data.img_path)/batch_size))\n",
    "val_batches_per_epoch = int(np.floor(len(test_data.img_path) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 67 Accuracy : 0.0 Loss : 2.633448362350464:   7%|▋         | 68/1000 [07:37<1:41:34,  6.54s/it] "
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "next_batch = tr_iterator.get_next()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    with tqdm(total = num_epochs) as pbar:\n",
    "        with tf.Session() as sess:\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "            # Loop over number of epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # Initialize iterator with the training dataset\n",
    "                sess.run(training_init_op)\n",
    "\n",
    "                train_acc = 0.\n",
    "                train_loss = 0.\n",
    "                train_count = 0\n",
    "                for step in range(train_batches_per_epoch):\n",
    "                    # get next batch of data\n",
    "                    img_batch, label_batch = sess.run(next_batch)\n",
    "\n",
    "                    # And run the training op\n",
    "                    acc,cost=sess.run([accuracy,loss], feed_dict={model.x: img_batch,\n",
    "                                                  model.y_gt: label_batch,\n",
    "                                                  keep_prob: 0.7})\n",
    "                    # Generate summary with the current batch of data and write to file\n",
    "                    if step % display_step == 0:\n",
    "                        s = sess.run(merged_summary, feed_dict={model.x: img_batch,\n",
    "                                                                model.y_gt: label_batch,\n",
    "                                                                keep_prob: 0.7})\n",
    "\n",
    "                        writer.add_summary(s, epoch*train_batches_per_epoch + step)\n",
    "                    train_acc += acc\n",
    "                    train_loss += cost\n",
    "                    train_count += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(\"Training : \"+str(epoch)+\n",
    "                                     \" Accuracy : \"+str(train_acc/train_count if train_count !=0 else 0)+\n",
    "                                     \" Loss : \"+str(train_loss/train_count if train_count !=0 else 0))\n",
    "                # Validate the model on the entire validation set\n",
    "\n",
    "                sess.run(validation_init_op)\n",
    "                test_acc = 0.\n",
    "                test_count = 0\n",
    "                for step in range(val_batches_per_epoch):\n",
    "\n",
    "                    img_batch, label_batch = sess.run(next_batch)\n",
    "                    acc = sess.run(accuracy, feed_dict={model.x: img_batch,\n",
    "                                                        model.y_yt: label_batch,\n",
    "                                                        keep_prob: 1.})\n",
    "                    test_acc += acc\n",
    "                    test_count += 1\n",
    "                if epoch % 20 == 0 and epoch!=0 and test_count!=0:\n",
    "                    print(\"Validation : \"+str(epoch+1)+\" Accuracy : \"+str(test_acc/test_count))\n",
    "\n",
    "                # save checkpoint of the model\n",
    "                checkpoint_name = os.path.join(checkpoint_path, 'model_epoch'+str(epoch+1)+'.ckpt')\n",
    "                save_path = saver.save(sess, checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
